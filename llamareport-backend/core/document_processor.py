"""
ç®€åŒ–çš„æ–‡æ¡£å¤„ç†å™¨
ä¸“æ³¨äºPDFæ–‡æ¡£çš„æ–‡æœ¬æå–å’Œé¢„å¤„ç†
"""

import os
import tempfile
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
from llama_index.core import Document
from llama_index.readers.file import PDFReader
import pdfplumber

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """ç®€åŒ–çš„æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.pdf_reader = PDFReader()
    
    def process_file(self, file_path: str, filename: str) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ä»¶å¹¶æå–å†…å®¹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            filename: æ–‡ä»¶å
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            if not Path(file_path).exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if not filename.lower().endswith('.pdf'):
                raise ValueError("ç›®å‰åªæ”¯æŒPDFæ–‡ä»¶")
            
            # ä½¿ç”¨LlamaIndexæå–æ–‡æ¡£
            documents = self.pdf_reader.load_data(Path(file_path))
            
            # ä½¿ç”¨pdfplumberæå–è¯¦ç»†å†…å®¹
            detailed_content = self._extract_detailed_content(file_path)
            
            # æ·»åŠ å…ƒæ•°æ®
            for i, doc in enumerate(documents):
                doc.metadata.update({
                    'filename': filename,
                    'page_number': i + 1,
                    'source': 'pdf_processor'
                })
            
            result = {
                'filename': filename,
                'documents': documents,
                'detailed_content': detailed_content,
                'page_count': len(detailed_content['pages']),
                'total_text_length': sum(len(doc.text) for doc in documents),
                'processing_method': 'simple_pdf_processor'
            }
            
            # ğŸ” æ‰“å°å¤„ç†ç»“æœçš„JSONç»“æ„
            import json
            print("=" * 80)
            print(f"ğŸ“„ æ–‡æ¡£å¤„ç†å®Œæˆ: {filename}")
            print("=" * 80)
            
            # æ‰“å°ä¸»è¦ç»“æ„ä¿¡æ¯
            print("ğŸ“Š ä¸»è¦ç»“æ„:")
            print(f"  - æ–‡ä»¶å: {result['filename']}")
            print(f"  - é¡µæ•°: {result['page_count']}")
            print(f"  - æ–‡æ¡£æ•°é‡: {len(result['documents'])}")
            print(f"  - æ€»æ–‡æœ¬é•¿åº¦: {result['total_text_length']}")
            print(f"  - å¤„ç†æ–¹å¼: {result['processing_method']}")
            
            # æ‰“å°è¯¦ç»†å†…å®¹ç»“æ„
            print("\nğŸ“‹ è¯¦ç»†å†…å®¹ç»“æ„:")
            print(f"  - é¡µé¢æ•°é‡: {len(detailed_content['pages'])}")
            print(f"  - å…ƒæ•°æ®: {list(detailed_content['metadata'].keys()) if detailed_content['metadata'] else 'æ— '}")
            
            # æ‰“å°æ¯é¡µçš„è¡¨æ ¼ä¿¡æ¯
            print("\nğŸ“Š è¡¨æ ¼ä¿¡æ¯:")
            for i, page in enumerate(detailed_content['pages'][:3]):  # åªæ˜¾ç¤ºå‰3é¡µ
                print(f"  ç¬¬{i+1}é¡µ: {len(page['tables'])}ä¸ªè¡¨æ ¼, {len(page['text'])}ä¸ªå­—ç¬¦")
                if page['tables']:
                    for j, table in enumerate(page['tables'][:2]):  # æ¯é¡µæœ€å¤šæ˜¾ç¤º2ä¸ªè¡¨æ ¼
                        print(f"    è¡¨æ ¼{j+1}: {table['rows']}è¡Œ x {table['cols']}åˆ—")
            
            # æ‰“å°å®Œæ•´çš„JSONç»“æ„ï¼ˆå¯é€‰ï¼Œæ³¨é‡Šæ‰é¿å…è¾“å‡ºè¿‡é•¿ï¼‰
            # print("\nğŸ” å®Œæ•´JSONç»“æ„:")
            # print(json.dumps(result, ensure_ascii=False, indent=2, default=str))
            
            print("=" * 80)
            
            logger.info(f"æˆåŠŸå¤„ç†æ–‡æ¡£: {filename}, é¡µæ•°: {result['page_count']}")
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {filename}: {str(e)}")
            raise
    
    def _extract_detailed_content(self, pdf_path: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨pdfplumberæå–è¯¦ç»†å†…å®¹
        """
        content = {
            'pages': [],
            'metadata': {},
            'total_pages': 0
        }
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                content['metadata'] = pdf.metadata or {}
                content['total_pages'] = len(pdf.pages)
                
                for i, page in enumerate(pdf.pages):
                    page_content = {
                        'page_number': i + 1,
                        'text': page.extract_text() or '',
                        'tables': [],
                        'images': len(page.images),
                        'width': page.width,
                        'height': page.height
                    }
                    
                    # æå–è¡¨æ ¼
                    tables = page.extract_tables()
                    if tables:
                        for j, table in enumerate(tables):
                            if table and len(table) > 0:
                                page_content['tables'].append({
                                    'table_id': f"page_{i+1}_table_{j+1}",
                                    'data': table,
                                    'rows': len(table),
                                    'cols': len(table[0]) if table[0] else 0
                                })
                    
                    content['pages'].append(page_content)
            
            # ğŸ” æ‰“å°è¯¦ç»†å†…å®¹æå–ç»“æœ
            print(f"\nğŸ“„ è¯¦ç»†å†…å®¹æå–å®Œæˆ:")
            print(f"  - æ€»é¡µæ•°: {content['total_pages']}")
            print(f"  - æå–é¡µé¢æ•°: {len(content['pages'])}")
            print(f"  - PDFå…ƒæ•°æ®: {len(content['metadata'])}ä¸ªå­—æ®µ")
            
            # ç»Ÿè®¡è¡¨æ ¼æ€»æ•°
            total_tables = sum(len(page['tables']) for page in content['pages'])
            print(f"  - æ€»è¡¨æ ¼æ•°: {total_tables}")
            
            # æ˜¾ç¤ºå‰å‡ é¡µçš„è¯¦ç»†ä¿¡æ¯
            for i, page in enumerate(content['pages'][:2]):  # åªæ˜¾ç¤ºå‰2é¡µ
                print(f"  ç¬¬{i+1}é¡µ: {len(page['text'])}å­—ç¬¦, {len(page['tables'])}è¡¨æ ¼, {page['images']}å›¾ç‰‡")
                    
        except Exception as e:
            logger.error(f"æå–è¯¦ç»†å†…å®¹å¤±è´¥: {str(e)}")
            raise
        
        return content
    
    def extract_text_chunks(self, documents: List[Document], chunk_size: int = 1024, chunk_overlap: int = 200) -> List[Document]:
        """
        å°†æ–‡æ¡£åˆ†å—å¤„ç†
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            chunk_size: å—å¤§å°
            chunk_overlap: é‡å å¤§å°
            
        Returns:
            åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        from llama_index.core.text_splitter import TokenTextSplitter
        
        text_splitter = TokenTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        chunked_docs = []
        for doc in documents:
            chunks = text_splitter.split_text(doc.text)
            for i, chunk in enumerate(chunks):
                chunk_doc = Document(
                    text=chunk,
                    metadata={
                        **doc.metadata,
                        'chunk_id': i,
                        'total_chunks': len(chunks)
                    }
                )
                chunked_docs.append(chunk_doc)
        
        logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆ: {len(documents)} -> {len(chunked_docs)} å—")
        return chunked_docs
    
    def validate_file(self, file_path: str, max_size: int = 50 * 1024 * 1024) -> bool:
        """
        éªŒè¯æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            max_size: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            path = Path(file_path)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not path.exists():
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = path.stat().st_size
            if file_size > max_size:
                logger.error(f"æ–‡ä»¶è¿‡å¤§: {file_size} > {max_size}")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            if not path.suffix.lower() == '.pdf':
                logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {path.suffix}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def get_document_summary(self, documents: List[Document]) -> Dict[str, Any]:
        """
        è·å–æ–‡æ¡£æ‘˜è¦ä¿¡æ¯
        """
        if not documents:
            return {}
        
        total_text = ' '.join([doc.text for doc in documents])
        
        summary = {
            'document_count': len(documents),
            'total_characters': len(total_text),
            'total_words': len(total_text.split()),
            'average_doc_length': sum(len(doc.text) for doc in documents) / len(documents),
            'metadata_keys': list(documents[0].metadata.keys()) if documents else []
        }
        
        return summary
