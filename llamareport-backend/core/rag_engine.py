"""
ç®€åŒ–çš„RAGå¼•æ“
ä¸“æ³¨äºæ–‡æ¡£ç´¢å¼•æ„å»ºå’Œæ™ºèƒ½é—®ç­”
"""

import os
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
from llama_index.core import Document, VectorStoreIndex, StorageContext, Settings
from llama_index.core.storage.docstore import SimpleDocumentStore
from llama_index.core.storage.index_store import SimpleIndexStore
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.llms.deepseek import DeepSeek
from llama_index.embeddings.openai import OpenAIEmbedding
import chromadb

# å¯¼å…¥Hybrid Retriever
from .hybrid_retriever import HybridRetriever

logger = logging.getLogger(__name__)

class RAGEngine:
    """ç®€åŒ–çš„RAGå¼•æ“"""
    
    def __init__(self, storage_dir: str = "./storage", collection_name: str = "documents"):
        self.storage_dir = Path(storage_dir)
        self.collection_name = collection_name
        self.index = None
        self.query_engine = None
        self.chroma_client = None
        self.chroma_collection = None
        
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        self.storage_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®LlamaIndex
        self.llama_index_ready = self._setup_llama_index()

        # åˆå§‹åŒ–ChromaDB
        if self.llama_index_ready:
            self._setup_chroma()
        else:
            logger.warning("âš ï¸ è·³è¿‡ChromaDBåˆå§‹åŒ– - LlamaIndexæœªå°±ç»ª")
        
        # åˆå§‹åŒ–Hybrid Retriever
        self.hybrid_retriever = HybridRetriever(storage_dir=str(self.storage_dir))
        self.use_hybrid_retriever = True  # é»˜è®¤å¯ç”¨æ··åˆæ£€ç´¢
    
    def _setup_llama_index(self):
        """è®¾ç½®LlamaIndexé…ç½®"""
        try:
            # è·å– API Keys
            openai_api_key = os.getenv("OPENAI_API_KEY")
            deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")

            if not openai_api_key:
                logger.warning("âš ï¸ OPENAI_API_KEYæœªè®¾ç½®ï¼ŒEmbeddingåŠŸèƒ½å°†å—é™")
                return False

            if not deepseek_api_key:
                logger.warning("âš ï¸ DEEPSEEK_API_KEYæœªè®¾ç½®ï¼Œå¯¹è¯åŠŸèƒ½å°†å—é™")
                return False

            # è®¾ç½®LLM - ä½¿ç”¨ DeepSeek ä¸“ç”¨é›†æˆ
            deepseek_model = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")

            Settings.llm = DeepSeek(
                model=deepseek_model,
                api_key=deepseek_api_key,
                temperature=0.1
            )

            logger.info(f"âœ… DeepSeek LLMé…ç½®æˆåŠŸ - æ¨¡å‹: {deepseek_model}")

            # è®¾ç½®åµŒå…¥æ¨¡å‹ - ç»§ç»­ä½¿ç”¨ OpenAI
            Settings.embed_model = OpenAIEmbedding(
                model="text-embedding-3-small",
                api_key=openai_api_key
            )

            logger.info("âœ… OpenAI Embeddingé…ç½®æˆåŠŸ")
            logger.info("âœ… LlamaIndexé…ç½®æˆåŠŸ (DeepSeek LLM + OpenAI Embedding)")
            return True

        except Exception as e:
            logger.error(f"âŒ LlamaIndexé…ç½®å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
    
    def _setup_chroma(self):
        """è®¾ç½®ChromaDB"""
        try:
            # ä½¿ç”¨æŒä¹…åŒ–å®¢æˆ·ç«¯
            chroma_persist_dir = self.storage_dir / "chroma"
            chroma_persist_dir.mkdir(exist_ok=True)
            
            self.chroma_client = chromadb.PersistentClient(path=str(chroma_persist_dir))
            
            # å°è¯•è·å–ç°æœ‰é›†åˆï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
            try:
                self.chroma_collection = self.chroma_client.get_collection(self.collection_name)
                logger.info(f"âœ… åŠ è½½ç°æœ‰ChromaDBé›†åˆ: {self.collection_name}")
            except:
                self.chroma_collection = self.chroma_client.create_collection(self.collection_name)
                logger.info(f"âœ… åˆ›å»ºæ–°çš„ChromaDBé›†åˆ: {self.collection_name}")
            
        except Exception as e:
            logger.error(f"âŒ ChromaDBè®¾ç½®å¤±è´¥: {str(e)}")
            raise

    def load_existing_index(self) -> bool:
        """ä»ç°æœ‰çš„ChromaDBé›†åˆåŠ è½½ç´¢å¼•"""
        try:
            if not self.llama_index_ready:
                logger.error("âŒ LlamaIndexæœªå°±ç»ªï¼Œæ— æ³•åŠ è½½ç´¢å¼•")
                return False

            if not self.chroma_collection:
                logger.error("âŒ ChromaDBé›†åˆæœªåˆå§‹åŒ–")
                return False

            # æ£€æŸ¥é›†åˆæ˜¯å¦æœ‰æ•°æ®
            collection_count = self.chroma_collection.count()
            if collection_count == 0:
                logger.warning("âš ï¸ ChromaDBé›†åˆä¸ºç©ºï¼Œæ— æ³•åŠ è½½ç´¢å¼•")
                return False

            # åˆ›å»ºå‘é‡å­˜å‚¨
            from llama_index.vector_stores.chroma import ChromaVectorStore
            from llama_index.core import StorageContext

            vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)

            # ä»å‘é‡å­˜å‚¨åŠ è½½ç´¢å¼•
            self.index = VectorStoreIndex.from_vector_store(vector_store)

            # åˆ›å»ºæŸ¥è¯¢å¼•æ“ - ä¼˜åŒ–é…ç½®
            from llama_index.core.prompts import PromptTemplate

            # è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œå¼ºè°ƒä½¿ç”¨è¡¨æ ¼æ•°æ®
            qa_prompt_tmpl = (
                "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡åˆ†æåŠ©æ‰‹ã€‚ä¸‹é¢æ˜¯ä»æ–‡æ¡£ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š\n\n"
                "{context_str}\n\n"
                "è¯·ä»”ç»†é˜…è¯»ä¸Šè¿°å†…å®¹ï¼Œç‰¹åˆ«æ³¨æ„å…¶ä¸­çš„è¡¨æ ¼æ•°æ®ã€‚å¦‚æœå†…å®¹ä¸­åŒ…å«Markdownæ ¼å¼çš„è¡¨æ ¼ï¼Œ"
                "è¯·åŠ¡å¿…åˆ†æè¡¨æ ¼ä¸­çš„å…·ä½“æ•°å€¼ã€‚\n\n"
                "ç”¨æˆ·é—®é¢˜ï¼š{query_str}\n\n"
                "å›ç­”è¦æ±‚ï¼š\n"
                "1. å¿…é¡»åŸºäºæ£€ç´¢åˆ°çš„å…·ä½“æ•°æ®å›ç­”ï¼Œç‰¹åˆ«æ˜¯è¡¨æ ¼ä¸­çš„æ•°å€¼\n"
                "2. å¦‚æœæ‰¾åˆ°ç›¸å…³æ•°æ®ï¼Œè¯·å¼•ç”¨å…·ä½“æ•°å­—å’Œæ¥æº\n"
                "3. å¦‚æœæ•°æ®ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜ç¼ºå°‘å“ªäº›ä¿¡æ¯\n"
                "4. å¯¹äºè¶‹åŠ¿åˆ†æï¼Œéœ€è¦å¯¹æ¯”ä¸åŒæ—¶æœŸçš„æ•°æ®\n\n"
                "è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼š"
            )
            qa_prompt = PromptTemplate(qa_prompt_tmpl)

            self.query_engine = self.index.as_query_engine(
                similarity_top_k=10,  # å¢åŠ æ£€ç´¢æ•°é‡
                response_mode="compact",  # ä½¿ç”¨compactæ¨¡å¼ä¿ç•™æ›´å¤šç»†èŠ‚
                text_qa_template=qa_prompt,
                verbose=True
            )

            logger.info(f"âœ… æˆåŠŸä»ChromaDBåŠ è½½ç´¢å¼•ï¼ŒåŒ…å« {collection_count} ä¸ªå‘é‡")
            return True

        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {str(e)}")
            return False
    
    def build_index(self, processed_documents: Dict[str, Any], extracted_tables: Dict[str, List[Dict]] = None) -> bool:
        """
        æ„å»ºå‘é‡ç´¢å¼•

        Args:
            processed_documents: å¤„ç†è¿‡çš„æ–‡æ¡£
            extracted_tables: æå–çš„è¡¨æ ¼ï¼ˆå¯é€‰ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸæ„å»ºç´¢å¼•
        """
        if not self.llama_index_ready:
            logger.error("âŒ LlamaIndexæœªå°±ç»ªï¼Œæ— æ³•æ„å»ºç´¢å¼•")
            return False

        try:
            all_documents = []
            
            # ğŸ” æ‰“å°ç´¢å¼•æ„å»ºå¼€å§‹ä¿¡æ¯
            print("\n" + "=" * 80)
            print("ğŸš€ å¼€å§‹æ„å»ºRAGç´¢å¼•")
            print("=" * 80)
            
            # å¤„ç†æ–‡æœ¬æ–‡æ¡£
            print("ğŸ“„ å¤„ç†æ–‡æœ¬æ–‡æ¡£:")
            text_doc_count = 0
            for doc_name, doc_data in processed_documents.items():
                if 'documents' in doc_data:
                    print(f"  - {doc_name}: {len(doc_data['documents'])}ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                    for doc in doc_data['documents']:
                        # æ·»åŠ å…ƒæ•°æ®
                        doc.metadata.update({
                            'source_file': doc_name,
                            'document_type': 'text_content'
                        })
                        all_documents.append(doc)
                        text_doc_count += 1
            
            print(f"  ğŸ“Š æ–‡æœ¬æ–‡æ¡£æ€»æ•°: {text_doc_count}")
            
            # å¤„ç†è¡¨æ ¼æ•°æ®
            table_doc_count = 0
            if extracted_tables:
                print("\nğŸ“Š å¤„ç†è¡¨æ ¼æ•°æ®:")
                for doc_name, tables in extracted_tables.items():
                    print(f"  - {doc_name}: {len(tables)}ä¸ªè¡¨æ ¼")
                    for table in tables:
                        # å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬
                        table_text = self._table_to_text(table)
                        
                        table_doc = Document(
                            text=table_text,
                            metadata={
                                'source_file': doc_name,
                                'document_type': 'table_data',
                                'table_id': table['table_id'],
                                'page_number': table['page_number'],
                                'is_financial': table.get('is_financial', False),
                                'importance_score': table.get('importance_score', 0.0)
                            }
                        )
                        all_documents.append(table_doc)
                        table_doc_count += 1
                        
                        # æ‰“å°è¡¨æ ¼è½¬æ¢ç¤ºä¾‹ï¼ˆå‰2ä¸ªï¼‰
                        if table_doc_count <= 2:
                            print(f"    ğŸ“‹ è¡¨æ ¼{table_doc_count}: {table['table_id']}")
                            print(f"       - è´¢åŠ¡è¡¨æ ¼: {'æ˜¯' if table.get('is_financial', False) else 'å¦'}")
                            print(f"       - é‡è¦æ€§: {table.get('importance_score', 0.0):.2f}")
                            print(f"       - æ–‡æœ¬é•¿åº¦: {len(table_text)}å­—ç¬¦")
                
                print(f"  ğŸ“Š è¡¨æ ¼æ–‡æ¡£æ€»æ•°: {table_doc_count}")
            else:
                print("\nğŸ“Š è¡¨æ ¼æ•°æ®: æ— ")
            
            if not all_documents:
                logger.warning("æ²¡æœ‰æ–‡æ¡£å¯ä»¥ç´¢å¼•")
                return False
            
            print(f"\nğŸ“ˆ ç´¢å¼•ç»Ÿè®¡:")
            print(f"  - æ€»æ–‡æ¡£æ•°: {len(all_documents)}")
            print(f"  - æ–‡æœ¬æ–‡æ¡£: {text_doc_count}")
            print(f"  - è¡¨æ ¼æ–‡æ¡£: {table_doc_count}")
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            print(f"\nğŸ”§ åˆ›å»ºå‘é‡å­˜å‚¨...")
            vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
            
            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            print(f"ğŸ”§ åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡...")
            storage_context = StorageContext.from_defaults(
                docstore=SimpleDocumentStore(),
                index_store=SimpleIndexStore(),
                vector_store=vector_store
            )
            
            # æ„å»ºç´¢å¼•
            print(f"ğŸ”§ æ„å»ºå‘é‡ç´¢å¼•...")
            self.index = VectorStoreIndex.from_documents(
                all_documents,
                storage_context=storage_context
            )
            
            # åˆ›å»ºæŸ¥è¯¢å¼•æ“ - ä¼˜åŒ–é…ç½®
            print(f"ğŸ”§ åˆ›å»ºæŸ¥è¯¢å¼•æ“...")
            from llama_index.core.prompts import PromptTemplate

            # è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œå¼ºè°ƒä½¿ç”¨è¡¨æ ¼æ•°æ®
            qa_prompt_tmpl = (
                "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡åˆ†æåŠ©æ‰‹ã€‚ä¸‹é¢æ˜¯ä»æ–‡æ¡£ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š\n\n"
                "{context_str}\n\n"
                "è¯·ä»”ç»†é˜…è¯»ä¸Šè¿°å†…å®¹ï¼Œç‰¹åˆ«æ³¨æ„å…¶ä¸­çš„è¡¨æ ¼æ•°æ®ã€‚å¦‚æœå†…å®¹ä¸­åŒ…å«Markdownæ ¼å¼çš„è¡¨æ ¼ï¼Œ"
                "è¯·åŠ¡å¿…åˆ†æè¡¨æ ¼ä¸­çš„å…·ä½“æ•°å€¼ã€‚\n\n"
                "ç”¨æˆ·é—®é¢˜ï¼š{query_str}\n\n"
                "å›ç­”è¦æ±‚ï¼š\n"
                "1. å¿…é¡»åŸºäºæ£€ç´¢åˆ°çš„å…·ä½“æ•°æ®å›ç­”ï¼Œç‰¹åˆ«æ˜¯è¡¨æ ¼ä¸­çš„æ•°å€¼\n"
                "2. å¦‚æœæ‰¾åˆ°ç›¸å…³æ•°æ®ï¼Œè¯·å¼•ç”¨å…·ä½“æ•°å­—å’Œæ¥æº\n"
                "3. å¦‚æœæ•°æ®ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜ç¼ºå°‘å“ªäº›ä¿¡æ¯\n"
                "4. å¯¹äºè¶‹åŠ¿åˆ†æï¼Œéœ€è¦å¯¹æ¯”ä¸åŒæ—¶æœŸçš„æ•°æ®\n\n"
                "è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼š"
            )
            qa_prompt = PromptTemplate(qa_prompt_tmpl)

            self.query_engine = self.index.as_query_engine(
                similarity_top_k=10,  # å¢åŠ æ£€ç´¢æ•°é‡
                response_mode="compact",  # ä½¿ç”¨compactæ¨¡å¼ä¿ç•™æ›´å¤šç»†èŠ‚
                text_qa_template=qa_prompt,
                verbose=True
            )
            
            # ğŸ” æ‰“å°ç´¢å¼•æ„å»ºå®Œæˆä¿¡æ¯
            print(f"\nâœ… ç´¢å¼•æ„å»ºå®Œæˆ!")
            print(f"  - æ–‡æ¡£æ€»æ•°: {len(all_documents)}")
            print(f"  - å‘é‡å­˜å‚¨: ChromaDB")
            print(f"  - é›†åˆåç§°: {self.collection_name}")
            print(f"  - æ£€ç´¢æ•°é‡: 10")
            print(f"  - å“åº”æ¨¡å¼: compact")
            print("=" * 80)
            
            # æ„å»ºHybrid Retrieverç´¢å¼•
            if self.use_hybrid_retriever:
                print("ğŸš€ æ„å»ºHybrid Retrieverç´¢å¼•...")
                hybrid_success = self.hybrid_retriever.build_hybrid_index(
                    processed_documents, extracted_tables
                )
                if hybrid_success:
                    print("âœ… Hybrid Retrieverç´¢å¼•æ„å»ºæˆåŠŸ")
                    hybrid_stats = self.hybrid_retriever.get_stats()
                    print(f"  - æ–‡æœ¬ç´¢å¼•: {hybrid_stats['text_collection_count']}ä¸ªæ–‡æ¡£")
                    print(f"  - è¡¨æ ¼ç´¢å¼•: {hybrid_stats['table_collection_count']}ä¸ªæ–‡æ¡£")
                    print(f"  - è´¢åŠ¡æŒ‡æ ‡: {hybrid_stats['financial_metrics_count']}ä¸ª")
                else:
                    print("âŒ Hybrid Retrieverç´¢å¼•æ„å»ºå¤±è´¥")
            
            logger.info(f"âœ… æˆåŠŸæ„å»ºç´¢å¼•ï¼ŒåŒ…å« {len(all_documents)} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºç´¢å¼•å¤±è´¥: {str(e)}")
            return False
    
    def _table_to_text(self, table: Dict[str, Any]) -> str:
        """å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬è¡¨ç¤º - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œç”Ÿæˆæ›´æ˜“äºLLMç†è§£çš„æ ¼å¼"""
        try:
            text_parts = []

            # æ·»åŠ è¡¨æ ¼åŸºæœ¬ä¿¡æ¯å’Œä¸Šä¸‹æ–‡
            text_parts.append("=" * 80)
            text_parts.append(f"ğŸ“Š è¡¨æ ¼æ•°æ® - {table['table_id']}")
            text_parts.append(f"ğŸ“„ æ¥æºé¡µç : ç¬¬{table['page_number']}é¡µ")

            # æ ‡æ³¨è¡¨æ ¼ç±»å‹
            if table.get('is_financial'):
                text_parts.append("ğŸ’° ç±»å‹: è´¢åŠ¡æ•°æ®è¡¨æ ¼")

            if table.get('summary'):
                text_parts.append(f"ğŸ“ è¡¨æ ¼æ‘˜è¦: {table['summary']}")

            text_parts.append("=" * 80)

            # æ·»åŠ è¡¨æ ¼æ•°æ® - ä½¿ç”¨Markdownè¡¨æ ¼æ ¼å¼
            if 'table_data' in table:
                table_data = table['table_data']
                columns = table_data['columns']
                data_rows = table_data['data']

                # ç”ŸæˆMarkdownè¡¨æ ¼
                text_parts.append("\n**è¡¨æ ¼å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰ï¼š**\n")

                # è¡¨å¤´
                header = "| " + " | ".join([str(col) for col in columns]) + " |"
                text_parts.append(header)

                # åˆ†éš”çº¿
                separator = "|" + "|".join(["---" for _ in columns]) + "|"
                text_parts.append(separator)

                # æ•°æ®è¡Œ - æ˜¾ç¤ºæ‰€æœ‰è¡Œæˆ–æœ€å¤š30è¡Œ
                max_rows = min(len(data_rows), 30)
                for i, row in enumerate(data_rows[:max_rows]):
                    row_str = "| " + " | ".join([str(cell) if cell else "" for cell in row]) + " |"
                    text_parts.append(row_str)

                if len(data_rows) > max_rows:
                    text_parts.append(f"\n... (è¡¨æ ¼å…±æœ‰ {len(data_rows)} è¡Œæ•°æ®ï¼Œä»¥ä¸Šæ˜¾ç¤ºå‰ {max_rows} è¡Œ)")
                else:
                    text_parts.append(f"\n(è¡¨æ ¼å…±æœ‰ {len(data_rows)} è¡Œæ•°æ®)")

                # æ·»åŠ æ•°æ®ç»Ÿè®¡ä¿¡æ¯
                text_parts.append(f"\n**è¡¨æ ¼ç»´åº¦**: {len(data_rows)} è¡Œ Ã— {len(columns)} åˆ—")

                # å¯¹äºè´¢åŠ¡è¡¨æ ¼ï¼Œæ·»åŠ ç‰¹åˆ«æç¤º
                if table.get('is_financial'):
                    text_parts.append("\nâš ï¸ **é‡è¦æç¤º**: è¿™æ˜¯è´¢åŠ¡æ•°æ®è¡¨æ ¼ï¼ŒåŒ…å«å…·ä½“çš„æ•°å€¼ä¿¡æ¯ã€‚è¯·ä»”ç»†åˆ†æè¡¨æ ¼ä¸­çš„æ•°æ®æ¥å›ç­”é—®é¢˜ã€‚")

            text_parts.append("=" * 80 + "\n")

            result_text = "\n".join(text_parts)
            
            # ğŸ” æ‰“å°è¡¨æ ¼è½¬æ¢ä¿¡æ¯ï¼ˆä»…å‰2ä¸ªè¡¨æ ¼ï¼‰
            if hasattr(self, '_table_count'):
                self._table_count += 1
            else:
                self._table_count = 1
                
            if self._table_count <= 2:
                print(f"    ğŸ”„ è¡¨æ ¼è½¬æ¢: {table['table_id']}")
                print(f"       - åŸå§‹æ•°æ®: {len(table.get('table_data', {}).get('data', []))}è¡Œ")
                print(f"       - è½¬æ¢åé•¿åº¦: {len(result_text)}å­—ç¬¦")
                print(f"       - åŒ…å«Markdown: {'æ˜¯' if '|' in result_text else 'å¦'}")

            return result_text

        except Exception as e:
            logger.error(f"è¡¨æ ¼è½¬æ–‡æœ¬å¤±è´¥: {str(e)}")
            return f"è¡¨æ ¼ {table.get('table_id', 'unknown')} (è½¬æ¢å¤±è´¥: {str(e)})"
    
    def query(self, question: str, context_filter: Optional[Dict] = None) -> Dict[str, Any]:
        """
        æŸ¥è¯¢RAGç³»ç»Ÿ

        Args:
            question: æŸ¥è¯¢é—®é¢˜
            context_filter: ä¸Šä¸‹æ–‡è¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            # ğŸ” æ‰“å°æŸ¥è¯¢å¼€å§‹ä¿¡æ¯
            print("\n" + "=" * 80)
            print("ğŸ” å¼€å§‹RAGæŸ¥è¯¢")
            print("=" * 80)
            print(f"ğŸ“ åŸå§‹é—®é¢˜: {question}")
            
            if not self.llama_index_ready:
                print("âŒ RAGç³»ç»Ÿæœªå°±ç»ª")
                return {
                    'answer': "RAGç³»ç»Ÿæœªå°±ç»ªï¼Œè¯·æ£€æŸ¥OPENAI_API_KEYé…ç½®ã€‚",
                    'sources': [],
                    'error': True
                }

            if not self.query_engine:
                print("ğŸ”„ å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•...")
                # å°è¯•ä»ç°æœ‰çš„ChromaDBé›†åˆåŠ è½½ç´¢å¼•
                if not self.load_existing_index():
                    print("âŒ æ— æ³•åŠ è½½ç°æœ‰ç´¢å¼•")
                    return {
                        'answer': "RAGç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆå¤„ç†æ–‡æ¡£ã€‚",
                        'sources': [],
                        'error': True
                    }
                print("âœ… æˆåŠŸåŠ è½½ç°æœ‰ç´¢å¼•")
            
            # å¢å¼ºæŸ¥è¯¢
            print("ğŸ”§ å¢å¼ºæŸ¥è¯¢...")
            enhanced_query = self._enhance_query(question, context_filter)
            print(f"ğŸ“ å¢å¼ºåæŸ¥è¯¢: {enhanced_query[:200]}...")
            
            # æ‰§è¡ŒæŸ¥è¯¢
            if self.use_hybrid_retriever and self.hybrid_retriever.text_index and self.hybrid_retriever.table_index:
                print("ğŸ” ä½¿ç”¨Hybrid Retrieveræ‰§è¡Œæ··åˆæ£€ç´¢...")
                # ä½¿ç”¨Hybrid Retrieverè¿›è¡Œæ£€ç´¢
                hybrid_results = self.hybrid_retriever.retrieve(question, top_k=10)
                
                if hybrid_results:
                    print(f"ğŸ“Š Hybrid Retrieveræ£€ç´¢ç»“æœ:")
                    print(f"  - æ£€ç´¢åˆ°æ–‡æ¡£: {len(hybrid_results)}ä¸ª")
                    print(f"  - å¹³å‡ç»¼åˆè¯„åˆ†: {sum(r['comprehensive_score'] for r in hybrid_results)/len(hybrid_results):.3f}")
                    print(f"  - ç­–ç•¥: {hybrid_results[0]['strategy']}")
                    
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n\n".join([r['document'].text for r in hybrid_results])
                    
                    # ä½¿ç”¨LLMç”Ÿæˆå›ç­”
                    response = self.query_engine.query(enhanced_query)
                    
                    # æå–æ¥æºä¿¡æ¯
                    sources = []
                    for result in hybrid_results:
                        sources.append({
                            'text': result['document'].text[:200] + "..." if len(result['document'].text) > 200 else result['document'].text,
                            'metadata': result['document'].metadata,
                            'score': result['comprehensive_score'],
                            'sim_score': result['sim_score'],
                            'metric_score': result['metric_score'],
                            'year_score': result['year_score']
                        })
                else:
                    print("âš ï¸ Hybrid Retrieveræœªæ‰¾åˆ°ç»“æœï¼Œå›é€€åˆ°ä¼ ç»Ÿæ£€ç´¢")
                    response = self.query_engine.query(enhanced_query)
                    sources = self._extract_sources(response)
            else:
                print("ğŸ” æ‰§è¡Œä¼ ç»Ÿå‘é‡æ£€ç´¢å’ŒLLMç”Ÿæˆ...")
                response = self.query_engine.query(enhanced_query)
                
                # æå–æ¥æºä¿¡æ¯
                print("ğŸ“š æå–æ¥æºä¿¡æ¯...")
                sources = self._extract_sources(response)
            
            result = {
                'answer': str(response),
                'sources': sources,
                'error': False,
                'original_question': question,
                'enhanced_query': enhanced_query
            }
            
            # ğŸ” æ‰“å°æŸ¥è¯¢ç»“æœä¿¡æ¯
            print(f"\nâœ… æŸ¥è¯¢å®Œæˆ!")
            print(f"  - æ£€ç´¢åˆ°æ¥æº: {len(sources)}ä¸ª")
            print(f"  - å›ç­”é•¿åº¦: {len(str(response))}å­—ç¬¦")
            print(f"  - æ¥æºç±»å‹: {[s.get('metadata', {}).get('document_type', 'unknown') for s in sources[:3]]}")
            print("=" * 80)
            
            logger.info(f"âœ… æŸ¥è¯¢æˆåŠŸ: {question[:50]}...")
            return result
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
            logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return {
                'answer': f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
                'sources': [],
                'error': True
            }
    
    def _enhance_query(self, question: str, context_filter: Optional[Dict] = None) -> str:
        """å¢å¼ºæŸ¥è¯¢ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œç¡®ä¿LLMä½¿ç”¨æ£€ç´¢åˆ°çš„æ•°æ®"""
        enhanced_parts = []

        # æ·»åŠ æ˜ç¡®çš„æŒ‡ä»¤
        enhanced_parts.append("ã€é‡è¦æŒ‡ä»¤ã€‘è¯·ä»”ç»†é˜…è¯»ä¸‹é¢æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼Œç‰¹åˆ«æ˜¯è¡¨æ ¼æ•°æ®ï¼Œå¹¶åŸºäºè¿™äº›å…·ä½“æ•°æ®æ¥å›ç­”é—®é¢˜ã€‚")
        enhanced_parts.append("å¦‚æœæ£€ç´¢åˆ°çš„å†…å®¹ä¸­åŒ…å«è¡¨æ ¼ï¼Œè¯·åŠ¡å¿…åˆ†æè¡¨æ ¼ä¸­çš„æ•°å€¼æ•°æ®ã€‚")
        enhanced_parts.append("")

        # æ·»åŠ åŸå§‹é—®é¢˜
        enhanced_parts.append(f"ã€ç”¨æˆ·é—®é¢˜ã€‘{question}")
        enhanced_parts.append("")

        # æ·»åŠ ä¸Šä¸‹æ–‡è¿‡æ»¤
        if context_filter:
            enhanced_parts.append("ã€æŸ¥è¯¢æ¡ä»¶ã€‘")
            if 'company' in context_filter:
                enhanced_parts.append(f"- å…¬å¸: {context_filter['company']}")

            if 'year' in context_filter:
                enhanced_parts.append(f"- å¹´ä»½: {context_filter['year']} å¹´")

            if 'document_type' in context_filter:
                enhanced_parts.append(f"- æ–‡æ¡£ç±»å‹: {context_filter['document_type']}")

            enhanced_parts.append("")

        # æ·»åŠ å›ç­”è¦æ±‚
        enhanced_parts.append("ã€å›ç­”è¦æ±‚ã€‘")
        enhanced_parts.append("1. å¿…é¡»ä½¿ç”¨æ£€ç´¢åˆ°çš„å…·ä½“æ•°æ®ï¼ˆç‰¹åˆ«æ˜¯è¡¨æ ¼ä¸­çš„æ•°å€¼ï¼‰")
        enhanced_parts.append("2. å¦‚æœæ•°æ®ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜ç¼ºå°‘å“ªäº›ä¿¡æ¯")
        enhanced_parts.append("3. æä¾›æ•°æ®æ¥æºï¼ˆé¡µç ã€è¡¨æ ¼IDç­‰ï¼‰")
        enhanced_parts.append("4. å¯¹äºè¶‹åŠ¿åˆ†æï¼Œéœ€è¦å¯¹æ¯”ä¸åŒæ—¶æœŸçš„æ•°æ®")

        return "\n".join(enhanced_parts)
    
    def _extract_sources(self, response) -> List[Dict[str, Any]]:
        """æå–æ¥æºä¿¡æ¯"""
        sources = []
        
        try:
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes:
                    source_info = {
                        'text': node.text[:200] + "..." if len(node.text) > 200 else node.text,
                        'metadata': node.metadata,
                        'score': getattr(node, 'score', 0.0)
                    }
                    sources.append(source_info)
        except Exception as e:
            logger.warning(f"æå–æ¥æºä¿¡æ¯å¤±è´¥: {str(e)}")
        
        return sources
    
    def get_similar_content(self, query: str, top_k: int = 5) -> List[Dict]:
        """è·å–ç›¸ä¼¼å†…å®¹"""
        try:
            if not self.index:
                return []
            
            retriever = self.index.as_retriever(similarity_top_k=top_k)
            nodes = retriever.retrieve(query)
            
            similar_content = []
            for node in nodes:
                content = {
                    'text': node.text,
                    'score': getattr(node, 'score', 0.0),
                    'metadata': node.metadata
                }
                similar_content.append(content)
            
            return similar_content
            
        except Exception as e:
            logger.error(f"è·å–ç›¸ä¼¼å†…å®¹å¤±è´¥: {str(e)}")
            return []
    
    def get_index_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not self.index:
                return {'status': 'not_initialized'}
            
            # è·å–æ–‡æ¡£æ•°é‡
            doc_count = len(self.index.docstore.docs)
            
            # è·å–é›†åˆä¿¡æ¯
            collection_count = self.chroma_collection.count() if self.chroma_collection else 0
            
            return {
                'status': 'ready',
                'document_count': doc_count,
                'vector_count': collection_count,
                'storage_dir': str(self.storage_dir),
                'collection_name': self.collection_name
            }
            
        except Exception as e:
            logger.error(f"è·å–ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {'status': 'error', 'error': str(e)}
    
    def clear_index(self) -> bool:
        """æ¸…ç©ºç´¢å¼•"""
        try:
            # åˆ é™¤ChromaDBé›†åˆ
            if self.chroma_collection:
                self.chroma_client.delete_collection(self.collection_name)
                self.chroma_collection = self.chroma_client.create_collection(self.collection_name)
            
            # é‡ç½®ç´¢å¼•
            self.index = None
            self.query_engine = None
            
            logger.info("âœ… ç´¢å¼•å·²æ¸…ç©º")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºç´¢å¼•å¤±è´¥: {str(e)}")
            return False
