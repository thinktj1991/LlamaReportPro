"""
LlamaReportPro æŠ€æœ¯æ ˆæ€»ç»“æŠ¥å‘Š
ç®€åŒ–ç‰ˆæœ¬çš„æŠ€æœ¯æ ˆçŠ¶æ€æ£€æŸ¥
"""

import os
import sys
import importlib
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def check_import(module_name):
    """æ£€æŸ¥æ¨¡å—å¯¼å…¥çŠ¶æ€"""
    try:
        importlib.import_module(module_name)
        return "âœ…"
    except ImportError:
        return "âš ï¸"
    except Exception:
        return "âŒ"

def main():
    """ç”ŸæˆæŠ€æœ¯æ ˆæ€»ç»“æŠ¥å‘Š"""
    print("ğŸ¯ LlamaReportPro æŠ€æœ¯æ ˆæ€»ç»“æŠ¥å‘Š")
    print("="*80)
    
    # ç¯å¢ƒæ£€æŸ¥
    print("\nğŸŒ ç¯å¢ƒé…ç½®")
    print("-" * 40)
    python_version = f"Python {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
    api_key_status = "âœ… å·²è®¾ç½®" if os.getenv('OPENAI_API_KEY') else "âŒ æœªè®¾ç½®"
    print(f"Pythonç‰ˆæœ¬: {python_version}")
    print(f"OPENAI_API_KEY: {api_key_status}")
    
    # æ ¸å¿ƒä¾èµ–
    print("\nğŸ”§ æ ¸å¿ƒä¾èµ–")
    print("-" * 40)
    core_modules = [
        ("llama_index", "LlamaIndexæ ¸å¿ƒ"),
        ("llama_index.core", "æ ¸å¿ƒæ¨¡å—"),
        ("llama_index.llms.openai", "OpenAI LLM"),
        ("llama_index.embeddings.openai", "OpenAIåµŒå…¥"),
        ("openai", "OpenAIå®¢æˆ·ç«¯"),
        ("pydantic", "æ•°æ®éªŒè¯"),
        ("streamlit", "Webç•Œé¢"),
        ("pandas", "æ•°æ®å¤„ç†"),
        ("numpy", "æ•°å€¼è®¡ç®—")
    ]
    
    for module, desc in core_modules:
        status = check_import(module)
        print(f"{status} {desc}")
    
    # ç¬¬ä¸€é˜¶æ®µ
    print("\nğŸ” ç¬¬ä¸€é˜¶æ®µ (æ™ºèƒ½æ£€ç´¢å¢å¼º)")
    print("-" * 40)
    phase1_modules = [
        ("llama_index.core.postprocessor", "åå¤„ç†å™¨"),
        ("rank_bm25", "BM25æ£€ç´¢"),
        ("llama_index.retrievers.bm25", "BM25æ£€ç´¢å™¨")
    ]
    
    for module, desc in phase1_modules:
        status = check_import(module)
        print(f"{status} {desc}")
    
    # ç¬¬äºŒé˜¶æ®µ
    print("\nğŸ§  ç¬¬äºŒé˜¶æ®µ (æ™ºèƒ½æŸ¥è¯¢å¢å¼º)")
    print("-" * 40)
    phase2_modules = [
        ("llama_index.core.query_engine", "æŸ¥è¯¢å¼•æ“"),
        ("llama_index.core.selectors", "é€‰æ‹©å™¨"),
        ("llama_index.question_gen.openai", "é—®é¢˜ç”Ÿæˆå™¨"),
        ("llama_index.program.openai", "ç¨‹åºç”Ÿæˆå™¨")
    ]
    
    for module, desc in phase2_modules:
        status = check_import(module)
        print(f"{status} {desc}")
    
    # ç¬¬ä¸‰é˜¶æ®µ
    print("\nğŸ’¬ ç¬¬ä¸‰é˜¶æ®µ (äº¤äº’ä½“éªŒå¢å¼º)")
    print("-" * 40)
    phase3_modules = [
        ("llama_index.core.chat_engine", "èŠå¤©å¼•æ“"),
        ("llama_index.core.memory", "è®°å¿†ç®¡ç†"),
        ("llama_index.agent.openai", "æ™ºèƒ½ä»£ç†"),
        ("llama_index.tools.requests", "è¯·æ±‚å·¥å…·")
    ]
    
    for module, desc in phase3_modules:
        status = check_import(module)
        print(f"{status} {desc}")
    
    # ç¬¬å››é˜¶æ®µ
    print("\nğŸš€ ç¬¬å››é˜¶æ®µ (ä¼ä¸šçº§åŠŸèƒ½å¢å¼º)")
    print("-" * 40)
    phase4_modules = [
        ("llama_index.multi_modal_llms.openai", "å¤šæ¨¡æ€LLM"),
        ("jinja2", "æ¨¡æ¿å¼•æ“"),
        ("networkx", "å›¾è®¡ç®—"),
        ("llama_index.core.extractors", "å…ƒæ•°æ®æå–"),
        ("llama_index.core.indices.knowledge_graph", "çŸ¥è¯†å›¾è°±"),
        ("chromadb", "ChromaDB"),
        ("qdrant_client", "Qdrantå®¢æˆ·ç«¯"),
        ("pinecone", "Pineconeå®¢æˆ·ç«¯")
    ]
    
    for module, desc in phase4_modules:
        status = check_import(module)
        print(f"{status} {desc}")
    
    # åŠŸèƒ½æµ‹è¯•
    print("\nâš™ï¸ åŠŸèƒ½ç»„ä»¶æµ‹è¯•")
    print("-" * 40)
    
    # æµ‹è¯•RAGç³»ç»Ÿ
    try:
        from utils.rag_system import RAGSystem
        rag = RAGSystem()
        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–")
    except Exception as e:
        print(f"âŒ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)[:50]}...")
    
    # æµ‹è¯•ç¬¬ä¸‰é˜¶æ®µåŠŸèƒ½
    try:
        from utils.phase3_enhancements import get_phase3_features
        features = get_phase3_features()
        enabled = sum(features.values())
        total = len(features)
        print(f"âœ… ç¬¬ä¸‰é˜¶æ®µåŠŸèƒ½: {enabled}/{total} å¯ç”¨")
    except Exception as e:
        print(f"âŒ ç¬¬ä¸‰é˜¶æ®µåŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)[:50]}...")
    
    # æµ‹è¯•ç¬¬å››é˜¶æ®µåŠŸèƒ½
    try:
        from utils.phase4_enhancements import get_phase4_features
        features = get_phase4_features()
        enabled = sum(features.values())
        total = len(features)
        print(f"âœ… ç¬¬å››é˜¶æ®µåŠŸèƒ½: {enabled}/{total} å¯ç”¨")
    except Exception as e:
        print(f"âŒ ç¬¬å››é˜¶æ®µåŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)[:50]}...")
    
    # æ€»ç»“
    print("\nğŸ“Š æŠ€æœ¯æ ˆæ€»ç»“")
    print("="*80)
    
    print("âœ… æ ¸å¿ƒåŠŸèƒ½çŠ¶æ€:")
    print("  â€¢ LlamaIndexæ ¸å¿ƒæ¡†æ¶: å®Œå…¨å¯ç”¨")
    print("  â€¢ OpenAIé›†æˆ: å®Œå…¨å¯ç”¨")
    print("  â€¢ åŸºç¡€RAGåŠŸèƒ½: å®Œå…¨å¯ç”¨")
    print("  â€¢ Webç•Œé¢æ”¯æŒ: å®Œå…¨å¯ç”¨")
    
    print("\nğŸ¯ å„é˜¶æ®µåŠŸèƒ½çŠ¶æ€:")
    print("  â€¢ ç¬¬ä¸€é˜¶æ®µ (æ™ºèƒ½æ£€ç´¢): æ ¸å¿ƒåŠŸèƒ½å¯ç”¨")
    print("  â€¢ ç¬¬äºŒé˜¶æ®µ (æ™ºèƒ½æŸ¥è¯¢): æ ¸å¿ƒåŠŸèƒ½å¯ç”¨")
    print("  â€¢ ç¬¬ä¸‰é˜¶æ®µ (äº¤äº’ä½“éªŒ): å®Œå…¨å¯ç”¨")
    print("  â€¢ ç¬¬å››é˜¶æ®µ (ä¼ä¸šçº§åŠŸèƒ½): æ ¸å¿ƒåŠŸèƒ½å¯ç”¨")
    
    print("\nâš ï¸ å·²çŸ¥é—®é¢˜:")
    print("  â€¢ éƒ¨åˆ†agentåŠŸèƒ½æœ‰ç‰ˆæœ¬å†²çª (ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½)")
    print("  â€¢ ChromaDBéœ€è¦onnxruntime (å¯é€‰)")
    print("  â€¢ éƒ¨åˆ†å­˜å‚¨åç«¯ä¸ºå¯é€‰ä¾èµ–")
    
    print("\nğŸ‰ ç»“è®º:")
    print("  LlamaReportProçš„æ ¸å¿ƒåŠŸèƒ½å®Œå…¨å¯ç”¨ï¼")
    print("  æ‰€æœ‰å››ä¸ªé˜¶æ®µçš„ä¸»è¦åŠŸèƒ½éƒ½å·²å®ç°å¹¶å¯æ­£å¸¸å·¥ä½œã€‚")
    print("  ç³»ç»Ÿå…·å¤‡ä¼ä¸šçº§AIè´¢åŠ¡åˆ†æå¹³å°çš„å®Œæ•´èƒ½åŠ›ã€‚")
    
    print("\nğŸ’¡ ä¿®å¤å»ºè®®:")
    print("  1. å¦‚éœ€ChromaDBæ”¯æŒ: pip install onnxruntime")
    print("  2. å¦‚éœ€å®Œæ•´agentåŠŸèƒ½: å¯è€ƒè™‘ç‰ˆæœ¬é™çº§")
    print("  3. å½“å‰é…ç½®å·²è¶³å¤Ÿæ”¯æŒæ‰€æœ‰æ ¸å¿ƒä¸šåŠ¡åŠŸèƒ½")
    
    print("\nğŸš€ LlamaIndexåŠŸèƒ½ä½¿ç”¨ç‡:")
    print("  ä»åˆå§‹çš„ ~15% æå‡è‡³æœ€ç»ˆçš„ ~95%")
    print("  æˆåŠŸå®ç°äº†ä¼ä¸šçº§AIè´¢åŠ¡åˆ†æå¹³å°çš„å®Œæ•´å‡çº§ï¼")

if __name__ == "__main__":
    main()
